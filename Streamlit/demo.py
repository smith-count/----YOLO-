from datetime import datetime
import zipfile
import shutil
import numpy as np
from ultralytics import YOLO
from PIL import Image
from io import BytesIO
import os
import threading
import time
import cv2
import streamlit as st
from util import convert_video_with_ffmpeg

# ======================
# è‡ªå®šä¹‰CSSæ ·å¼
# ======================
st.markdown("""
<style>
    /* ä¸»å®¹å™¨æ ·å¼ */
    .main-container {
        background: linear-gradient(145deg, #f8f9fa 0%, #e9ecef 100%);
        border-radius: 15px;
        padding: 2rem;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }

    /* åŠ¨æ€è¾¹æ¡†åŠ¨ç”» */
    @keyframes border-glow {
        0% { box-shadow: 0 0 10px rgba(76,175,80,0.3); }
        50% { box-shadow: 0 0 15px rgba(33,150,243,0.5); }
        100% { box-shadow: 0 0 10px rgba(76,175,80,0.3); }
    }

    .section-card {
        border: 1px solid #dee2e6;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        background: white;
        animation: border-glow 3s ease-in-out infinite;
    }

    /* æŒ‰é’®ç¾åŒ– */
    .stButton>button {
        background: linear-gradient(45deg, #4CAF50, #2196F3) !important;
        color: white !important;
        border: none !important;
        border-radius: 25px !important;
        padding: 0.8rem 1.5rem !important;
        font-weight: 600 !important;
        transition: all 0.3s !important;
    }

    .stButton>button:hover {
        transform: scale(1.05);
        box-shadow: 0 4px 15px rgba(0,0,0,0.2) !important;
    }

    /* ä¾§è¾¹æ ç¾åŒ– */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #2c3e50, #34495e) !important;
        box-shadow: 2px 0 15px rgba(0,0,0,0.1);
    }

    .sidebar-title {
        color: #fff !important;
        font-size: 1.5rem !important;
        padding: 1rem 0;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–å…¨å±€å˜é‡
MODEL_PATHS = {
    "YOLOv11":"D:\Python\graduate_design\Model\yolo11n.pt",
    "YOLO_detect_animals":r"D:\Python\graduate_design\Model\runs\train\exp\weights\best.pt", #
}

current_model = YOLO(r"D:\Python\graduate_design\Model\yolo11n.pt")# é»˜è®¤
conf_threshold = 0
iou_threshold = 0


# åŠ è½½æ¨¡å‹
def load_model(model_name):
    global current_model
    model_path = MODEL_PATHS.get(model_name)
    if not model_path:
        st.error("æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼")
        return None
    try:
        model = YOLO(model_path)
        st.success(f"æˆåŠŸåŠ è½½æ¨¡å‹ï¼š{model_name}")
        return model
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{e}")
        return None

# # å®æ—¶æ‘„åƒå¤´æ£€æµ‹
# def realtime_detection():
#     global current_model
#     st.header("ğŸ“¹ å®æ—¶æ‘„åƒå¤´æ£€æµ‹")
#
#     # åˆå§‹åŒ–æ‘„åƒå¤´
#     camera_placeholder = st.empty()
#     stop_button = st.button("åœæ­¢æ£€æµ‹")
#
#     # è·å–æ‘„åƒå¤´è®¾å¤‡ï¼ˆé»˜è®¤0ï¼‰
#     cap = cv2.VideoCapture(0)
#     if not cap.isOpened():
#         st.error("æ— æ³•è®¿é—®æ‘„åƒå¤´")
#         return
#
#     try:
#         while cap.isOpened() and not stop_button:
#             # è¯»å–è§†é¢‘å¸§
#             ret, frame = cap.read()
#             if not ret:
#                 st.warning("è§†é¢‘æµä¸­æ–­")
#                 break
#
#             # YOLOæ¨ç†
#             results = current_model.track(
#                 source=frame,
#                 conf=conf_threshold,
#                 iou=iou_threshold,
#                 persist=True,  # ç»´æŒè·Ÿè¸ªçŠ¶æ€
#                 verbose=False
#             )
#
#             # å®æ—¶æ ‡æ³¨
#             annotated_frame = results[0].plot()  # ä½¿ç”¨å†…ç½®ç»˜å›¾æ–¹æ³•
#
#             # æ˜¾ç¤ºå®æ—¶ç”»é¢
#             camera_placeholder.image(annotated_frame,
#                                      channels="BGR",
#                                      caption="å®æ—¶æ£€æµ‹ç”»é¢")
#
#             # æ§åˆ¶å¸§ç‡ï¼ˆæ ¹æ®ç¡¬ä»¶è°ƒæ•´ï¼‰
#             time.sleep(0.01)  # çº¦100FPS
#
#     except Exception as e:
#         st.error(f"æ£€æµ‹å¼‚å¸¸: {str(e)}")
#     finally:
#         cap.release()
#         st.info("æ‘„åƒå¤´å·²é‡Šæ”¾")
#
#
# def draw_detections(frame, results, class_names) -> np.ndarray:
#     """åœ¨å¸§ä¸Šç»˜åˆ¶æ£€æµ‹æ¡†å’Œæ ‡ç­¾"""
#     annotated = frame.copy()
#     color = (0, 255, 0)  # ç»¿è‰²è¾¹æ¡†
#
#     for result in results:
#         for box in result.boxes:
#             # è§£ææ£€æµ‹ç»“æœ
#             x1, y1, x2, y2 = map(int, box.xyxy[0].tolist())
#             conf = box.conf.item()
#             cls_id = int(box.cls.item())
#
#             # ç»˜åˆ¶å…ƒç´ 
#             cv2.rectangle(annotated, (x1, y1), (x2, y2), color, 2)
#             label = f"{class_names[cls_id]} {conf:.2f}"
#             cv2.putText(annotated, label, (x1, y1 - 10),
#                         cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 1)
#
#     return annotated

# å›¾ç‰‡æ£€æµ‹
def image_detection():
    global current_model
    st.header("å›¾ç‰‡æ£€æµ‹")
    #st.markdown('<div class="dynamic-border">', unsafe_allow_html=True)

    # ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶", type=["jpg", "jpeg", "png"])

    if uploaded_file:
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
        col1, col2 = st.columns(2)

        # æ‰“å¼€åŸå§‹å›¾åƒ
        original_image = Image.open(uploaded_file)

        # å·¦ä¾§æ˜¾ç¤ºåŸå§‹å›¾ç‰‡
        with col1:
            st.subheader("åŸå§‹å›¾ç‰‡")
            st.image(original_image, use_container_width=True)

        # å³ä¾§æ˜¾ç¤ºå¤„ç†è¿›åº¦å’Œç»“æœ
        with col2:
            st.subheader("æ£€æµ‹ç»“æœ")

            # æ·»åŠ å¤„ç†ä¸­çš„æ—‹è½¬åŠ¨ç”»
            with st.spinner("YOLOæ¨¡å‹æ­£åœ¨å¤„ç†ä¸­..."):
                # ä½¿ç”¨YOLOæ¨¡å‹è¿›è¡Œæ¨ç†
                results = current_model(
                    source=original_image,
                    conf=conf_threshold,
                    iou=iou_threshold,
                    save=False
                )

                # å°†æ¨ç†ç»“æœç»˜åˆ¶åˆ°å›¾åƒä¸Š
                annotated_image = results[0].plot()  # è¿”å›å¸¦æ³¨é‡Šçš„å›¾åƒï¼ˆnumpyæ•°ç»„ï¼‰
                annotated_image = Image.fromarray(annotated_image[..., ::-1])  # è½¬æ¢ä¸ºPILå›¾åƒ

                # æ˜¾ç¤ºå¤„ç†åçš„å›¾åƒ
                st.image(annotated_image, use_container_width=True)

                # æ˜¾ç¤ºæ£€æµ‹ç»Ÿè®¡ä¿¡æ¯
                num_objects = len(results[0].boxes)
                st.success(f"æ£€æµ‹åˆ° {num_objects} ä¸ªç›®æ ‡")

                # å°†å¤„ç†åçš„å›¾åƒè½¬æ¢ä¸ºå­—èŠ‚æµä¾›ä¸‹è½½
                buffered = BytesIO()
                annotated_image.save(buffered, format="JPEG")
                img_bytes = buffered.getvalue()

                # æ·»åŠ å³ä¸‹è§’ä¸‹è½½æŒ‰é’®ï¼ˆä½¿ç”¨CSSå›ºå®šä½ç½®ï¼‰
                st.markdown(
                    """
                    <style>
                    .download-btn {
                        position: fixed;
                        bottom: 20px;
                        right: 20px;
                        z-index: 999;
                    }
                    </style>
                    """,
                    unsafe_allow_html=True
                )

                # å¯¼å‡ºæŒ‰é’®
                st.download_button(
                    label="ğŸ“¥ å¯¼å‡ºå¤„ç†åçš„å›¾ç‰‡",
                    data=img_bytes,
                    file_name="processed_image.jpg",
                    mime="image/jpeg",
                    key="download_image",
                    help="ç‚¹å‡»ä¸‹è½½YOLOå¤„ç†åçš„å›¾ç‰‡",
                    use_container_width=True,
                    on_click=lambda: st.toast("å›¾ç‰‡å¯¼å‡ºæˆåŠŸï¼"),
                    type="primary"
                )

    st.markdown('</div>', unsafe_allow_html=True)

def video_detection():
    global current_model
    st.header("ğŸ¥ è§†é¢‘æ£€æµ‹ç³»ç»Ÿ")

    # ä½¿ç”¨å®¹å™¨åˆ’åˆ†ä¸»è¦åŠŸèƒ½åŒºåŸŸ
    upload_container = st.container(border=True)
    with upload_container:
        # ä¸Šä¼ åŒºåŸŸ
        st.subheader("1. è§†é¢‘ä¸Šä¼ ")
        uploaded_file = st.file_uploader("é€‰æ‹©å¾…æ£€æµ‹è§†é¢‘æ–‡ä»¶",
                                         type=["mp4", "avi", "mov"],
                                         label_visibility="collapsed",
                                         help="æ”¯æŒ MP4/AVI/MOV æ ¼å¼ï¼Œæœ€å¤§æ–‡ä»¶å¤§å° 500MB")

    if uploaded_file:
        # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_video_path = "temp_uploaded_video.mp4"
        with open(temp_video_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # è§†é¢‘é¢„è§ˆå’Œå¤„ç†åŒºåŸŸ
        process_container = st.container(border=True)
        with process_container:
            st.subheader("2. è§†é¢‘å¤„ç†")

            # åˆ›å»ºå¯¹æ¯”å¸ƒå±€
            col1, col2 = st.columns([0.45, 0.55], gap="large")

            with col1:
                # åŸå§‹è§†é¢‘é¢„è§ˆ
                st.markdown("**åŸå§‹è§†é¢‘é¢„è§ˆ**")
                st.video(temp_video_path)

            with col2:
                # å¤„ç†çŠ¶æ€åŒºåŸŸ
                status_col1, status_col2 = st.columns([0.7, 0.3])
                with status_col1:
                    st.markdown("**å¤„ç†çŠ¶æ€**")
                    progress_placeholder = st.empty()

        # å¤„ç†ç»“æœåŒºåŸŸ
        processed_video_placeholder = st.empty()
        export_placeholder = st.empty()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.join("temp_results", "processed_video")
        os.makedirs(output_dir, exist_ok=True)
        # ç¡®å®šè¾“å‡ºè§†é¢‘è·¯å¾„
        output_video_name = "output_" + os.path.basename(temp_video_path)
        processed_temp_video_path = os.path.join(output_dir, "temp_processed_video.avi")  # YOLO è¾“å‡ºä¸´æ—¶è§†é¢‘
        final_processed_video_path = os.path.join(output_dir, output_video_name)

        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            cap = cv2.VideoCapture(temp_video_path)
            if not cap.isOpened():
                raise ValueError("æ— æ³•æ‰“å¼€ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶")
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            cap.release()

            # å¤„ç†è¿›åº¦å’ŒçŠ¶æ€ç®¡ç†
            processed_frames = 0
            lock = threading.Lock()

            def process_video():
                nonlocal processed_frames
                try:
                    # æ¨¡æ‹Ÿ YOLO æ¨ç†å’Œä¿å­˜è§†é¢‘
                    results = current_model.track(
                        source=temp_video_path,
                        conf=conf_threshold,
                        iou=iou_threshold,
                        stream=True,
                        imgsz=(width, height),
                        save=False,
                        project=None,
                        name=None,
                        verbose=False
                    )

                    # æ‰‹åŠ¨ä¿å­˜ YOLO å¤„ç†åçš„ä¸´æ—¶è§†é¢‘
                    fourcc = cv2.VideoWriter_fourcc(*'XVID')  # ä½¿ç”¨ AVI æ ¼å¼ä¿å­˜ä¸´æ—¶è§†é¢‘
                    out = cv2.VideoWriter(processed_temp_video_path, fourcc, fps, (width, height))

                    for frame_result in results:
                        processed_frame = frame_result.plot()  # ç¤ºä¾‹ï¼šç»˜åˆ¶æ£€æµ‹ç»“æœåˆ°å¸§
                        out.write(processed_frame)

                        # æ›´æ–°å¤„ç†è¿›åº¦ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰
                        with lock:
                            processed_frames += 1

                    out.release()
                except Exception as e:
                    st.error(f"è§†é¢‘å¤„ç†å†…éƒ¨é”™è¯¯: {str(e)}")

            # å¯åŠ¨å¤„ç†çº¿ç¨‹
            process_thread = threading.Thread(target=process_video)
            process_thread.start()

            # å®æ—¶æ›´æ–°è¿›åº¦
            while process_thread.is_alive():
                with lock:
                    progress = min(100, int((processed_frames / total_frames) * 100) if total_frames > 0 else 0)
                progress_placeholder.progress(
                    progress,
                    text=f"â– å¤„ç†è¿›åº¦: {progress}% | å·²å¤„ç† {processed_frames}/{total_frames} å¸§"
                )
                time.sleep(0.1)

            process_thread.join()

            # è§†é¢‘æ ¼å¼è½¬æ¢
            if os.path.exists(processed_temp_video_path):
                convert_video_with_ffmpeg(processed_temp_video_path, final_processed_video_path)

            # æ˜¾ç¤ºå¤„ç†ç»“æœ
            with process_container:
                with col2:
                    if os.path.exists(final_processed_video_path):
                        processed_video_placeholder.markdown("**æ£€æµ‹ç»“æœé¢„è§ˆ**")
                        processed_video_placeholder.video(final_processed_video_path)

                        # å¯¼å‡ºæŒ‰é’®æ ·å¼ä¼˜åŒ–
                        with export_placeholder:
                            with open(final_processed_video_path, "rb") as f:
                                st.download_button(
                                    label="â¬‡ï¸ å¯¼å‡ºæ£€æµ‹ç»“æœè§†é¢‘",
                                    data=f,
                                    file_name=output_video_name,
                                    mime="video/mp4",
                                    use_container_width=True,
                                    type="primary"
                                )

        except Exception as e:
            st.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", icon="ğŸš¨")

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_video_path):
                os.remove(temp_video_path)
            if os.path.exists(processed_temp_video_path):
                os.remove(processed_temp_video_path)

def model_usage():
    st.title("ğŸ¦ æ™ºèƒ½åŠ¨ç‰©è¯†åˆ«ç³»ç»Ÿ")
    st.markdown("---")

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.markdown('<h1 class="sidebar-title">âš™ï¸ æ§åˆ¶é¢æ¿</h1>', unsafe_allow_html=True)

        # æ¨¡å‹é€‰æ‹©
        model_name = st.selectbox(
            "**é€‰æ‹©æ£€æµ‹æ¨¡å‹**",
            list(MODEL_PATHS.keys()),
            index=0,
            help="é€‰æ‹©é¢„è®­ç»ƒçš„YOLOæ¨¡å‹"
        )
        current_model = load_model(model_name)

        # å‚æ•°è®¾ç½®
        st.markdown("---")
        st.markdown("**âš–ï¸ æ£€æµ‹å‚æ•°**")
        conf_threshold = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, 0.25, step=0.01)
        iou_threshold = st.slider("IoU é˜ˆå€¼", 0.0, 1.0, 0.5, step=0.01)

        # è®¾å¤‡è®¾ç½®
        st.markdown("---")
        st.markdown("**ğŸ“· è¾“å…¥æºè®¾ç½®**")
        camera_devices = ["é»˜è®¤æ‘„åƒå¤´", "å¤–éƒ¨è®¾å¤‡1", "å¤–éƒ¨è®¾å¤‡2"]
        selected_camera = st.selectbox("è§†é¢‘è¾“å…¥æº", camera_devices)

        # åŠŸèƒ½å¯¼èˆª
        st.markdown("---")
        st.markdown("**ğŸ¯ æ£€æµ‹æ¨¡å¼**")
        task = st.radio("", ["å›¾ç‰‡æ£€æµ‹", "è§†é¢‘æ£€æµ‹", "å®æ—¶æ£€æµ‹"], index=0)

    # ä¸»å†…å®¹åŒºåŸŸ
    with st.container():
        #st.markdown('<div class="main-container">', unsafe_allow_html=True)

        if task == "å›¾ç‰‡æ£€æµ‹":
            with st.container():
                # st.markdown('<div class="section-card">', unsafe_allow_html=True)
                st.header("ğŸ“¸ å›¾åƒæ£€æµ‹")

                # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
                uploaded_file = st.file_uploader(
                    "ä¸Šä¼ å¾…æ£€æµ‹å›¾ç‰‡",
                    type=["jpg", "jpeg", "png"],
                    help="æ”¯æŒJPG/JPEG/PNGæ ¼å¼ï¼Œæœ€å¤§å°ºå¯¸5MB"
                )

                if uploaded_file:
                    cols = st.columns([1, 1], gap="large")
                    with cols[0]:
                        st.markdown("#### åŸå§‹å›¾åƒ")
                        st.image(uploaded_file, use_container_width=True)

                    with cols[1]:
                        with st.spinner("ğŸ” æ­£åœ¨æ£€æµ‹ä¸­..."):
                            # æ¨¡å‹æ¨ç†
                            results = current_model.predict(
                                source=Image.open(uploaded_file),
                                conf=conf_threshold,
                                iou=iou_threshold
                            )
                            annotated_image = results[0].plot()[:, :, ::-1]

                            st.markdown("#### æ£€æµ‹ç»“æœ")
                            st.image(annotated_image, use_container_width=True)

                            # ç»“æœç»Ÿè®¡
                            num_objects = len(results[0].boxes)
                            st.success(f"âœ… æ£€æµ‹åˆ° {num_objects} ä¸ªç›®æ ‡")

                            # å¯¼å‡ºåŠŸèƒ½
                            buf = BytesIO()
                            Image.fromarray(annotated_image).save(buf, format="PNG")
                            st.download_button(
                                label="ğŸ’¾ ä¿å­˜æ£€æµ‹ç»“æœ",
                                data=buf.getvalue(),
                                file_name="detection_result.png",
                                mime="image/png",
                                use_container_width=True
                            )
                st.markdown('</div>', unsafe_allow_html=True)

        elif task == "è§†é¢‘æ£€æµ‹":
            # with st.container():
            #     st.markdown('<div class="section-card">', unsafe_allow_html=True)
            #     st.header("ğŸ¥ è§†é¢‘æ£€æµ‹")
            #
            #     # æ–‡ä»¶ä¸Šä¼ åŒºåŸŸ
            #     uploaded_file = st.file_uploader(
            #         "ä¸Šä¼ å¾…æ£€æµ‹è§†é¢‘",
            #         type=["mp4", "avi"],
            #         help="æ”¯æŒMP4/AVIæ ¼å¼ï¼Œæœ€å¤§å°ºå¯¸500MB"
            #     )
            #
            #     if uploaded_file:
            #         # è§†é¢‘é¢„è§ˆä¸å¤„ç†
            #         video_path = f"temp_{uploaded_file.name}"
            #         with open(video_path, "wb") as f:
            #             f.write(uploaded_file.getbuffer())
            #
            #         cols = st.columns([1, 1], gap="large")
            #         with cols[0]:
            #             st.markdown("#### åŸå§‹è§†é¢‘")
            #             st.video(uploaded_file)
            #
            #         with cols[1]:
            #             st.markdown("#### å¤„ç†è¿›åº¦")
            #             progress_bar = st.progress(0)
            #             status_text = st.empty()
            #
            #             # æ¨¡æ‹Ÿå¤„ç†è¿‡ç¨‹
            #             for percent in range(100):
            #                 time.sleep(0.02)
            #                 progress_bar.progress(percent + 1)
            #                 status_text.text(f"â– å¤„ç†è¿›åº¦: {percent + 1}%")
            #
            #             st.success("âœ… å¤„ç†å®Œæˆï¼")
            #             st.download_button(
            #                 label="ğŸ“¥ ä¸‹è½½ç»“æœè§†é¢‘",
            #                 data=open(video_path, "rb"),
            #                 file_name="processed_video.mp4",
            #                 use_container_width=True
            #             )
            #     st.markdown('</div>', unsafe_allow_html=True)
            video_detection()

        elif task == "å®æ—¶æ£€æµ‹":
            with st.container():
                #st.markdown('<div class="section-card">', unsafe_allow_html=True)
                st.header("ğŸŒ å®æ—¶æ£€æµ‹")

                # æ‘„åƒå¤´æ§åˆ¶
                if st.button("ğŸš€ å¯åŠ¨å®æ—¶æ£€æµ‹", use_container_width=True):
                    cap = cv2.VideoCapture(0)
                    frame_placeholder = st.empty()
                    stop_button = st.button("ğŸ›‘ åœæ­¢æ£€æµ‹", use_container_width=True)

                    while cap.isOpened() and not stop_button:
                        ret, frame = cap.read()
                        if not ret:
                            st.error("æ— æ³•è·å–è§†é¢‘æµ")
                            break

                        # å®æ—¶æ¨ç†
                        results = current_model.track(
                            frame,
                            conf=conf_threshold,
                            iou=iou_threshold
                        )
                        annotated_frame = results[0].plot()
                        frame_placeholder.image(annotated_frame, channels="BGR")

                    cap.release()
                st.markdown('</div>', unsafe_allow_html=True)

        st.markdown('</div>', unsafe_allow_html=True)

    # # æ˜¾ç¤ºç¤ºä¾‹å›¾ç‰‡
    # image = Image.open('D:\Python\graduate_design\img.png')  # ç¡®ä¿ 'img.png' æ–‡ä»¶å­˜åœ¨
    # st.image(image, caption='demo', use_container_width=True)

def model_train():
    # é¡µé¢è®¾ç½®
    # st.set_page_config(page_title="YOLOæ¨¡å‹è®­ç»ƒå¹³å°", layout="wide")

    # æ ‡é¢˜
    st.title("åŸºäºYOLOçš„åŠ¨ç‰©è¯†åˆ«æ¨¡å‹è®­ç»ƒ")
    st.markdown("---")

    # åˆå§‹åŒ–ä¼šè¯çŠ¶æ€
    if 'training' not in st.session_state:
        st.session_state.training = False
    if 'process' not in st.session_state:
        st.session_state.process = None
    if 'selected_dataset' not in st.session_state:
        st.session_state.selected_dataset = None
    if 'dataset_stats' not in st.session_state:
        st.session_state.dataset_stats = {"train": 0, "val": 0}


    # ä¾§è¾¹æ  - å¯¼èˆª
    st.sidebar.title("å‚æ•°è®¾ç½®")
    page = st.sidebar.radio("é€‰æ‹©é¡µé¢", ["æ•°æ®é›†é…ç½®", "æ¨¡å‹è®­ç»ƒ", "è®­ç»ƒç›‘æ§"])

    # æ•°æ®é›†é…ç½®é¡µé¢
    if page == "æ•°æ®é›†é…ç½®":
        with st.container():
            st.markdown('<div class="dynamic-border">', unsafe_allow_html=True)
            st.header("ğŸ“ æ•°æ®é›†é…ç½®")

            # æ•°æ®é›†ä¸Šä¼ éƒ¨åˆ†
            with st.expander("ä¸Šä¼ æ•°æ®é›†", expanded=True):
                uploaded_file = st.file_uploader("ä¸Šä¼ ZIPæ ¼å¼çš„æ•°æ®é›†", type=["zip"])

                if uploaded_file is not None:
                    # åˆ›å»ºä¸´æ—¶ç›®å½•
                    temp_dir = "temp_dataset"
                    os.makedirs(temp_dir, exist_ok=True)

                    # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
                    zip_path = os.path.join(temp_dir, uploaded_file.name)
                    with open(zip_path, "wb") as f:
                        f.write(uploaded_file.getbuffer())

                    # è§£å‹æ–‡ä»¶
                    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                        zip_ref.extractall(temp_dir)

                    # æ£€æŸ¥YOLOæ ¼å¼
                    st.info("æ£€æŸ¥æ•°æ®é›†ç»“æ„...")
                    required_folders = ["images", "labels"]
                    required_splits = ["train", "val"]

                    valid_structure = True
                    for folder in required_folders:
                        folder_path = os.path.join(temp_dir, folder)
                        if not os.path.exists(folder_path):
                            valid_structure = False
                            st.error(f"âŒ ç¼ºå°‘ {folder} æ–‡ä»¶å¤¹")
                            break
                        for split in required_splits:
                            split_path = os.path.join(folder_path, split)
                            if not os.path.exists(split_path):
                                valid_structure = False
                                st.error(f"âŒ åœ¨ {folder} ä¸­ç¼ºå°‘ {split} å­æ–‡ä»¶å¤¹")
                                break

                    if valid_structure:
                        st.success("âœ… æ•°æ®é›†ç»“æ„éªŒè¯é€šè¿‡ (YOLOæ ¼å¼)")

                        # è®¡ç®—æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
                        train_images = len(os.listdir(os.path.join(temp_dir, "images", "train")))
                        val_images = len(os.listdir(os.path.join(temp_dir, "images", "val")))
                        st.session_state.dataset_stats = {"train": train_images, "val": val_images}

                        # æ˜¾ç¤ºåŠ¨æ€ç»Ÿè®¡ä¿¡æ¯
                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("è®­ç»ƒå›¾åƒæ•°é‡", train_images)
                        with col2:
                            st.metric("éªŒè¯å›¾åƒæ•°é‡", val_images)

                        # ä¿å­˜æ•°æ®é›†é€‰é¡¹
                        dataset_name = st.text_input("è¾“å…¥æ•°æ®é›†åç§°", "my_dataset")
                        if st.button("ğŸ’¾ ä¿å­˜æ•°æ®é›†"):
                            if dataset_name:
                                dataset_dir = os.path.join("datasets", dataset_name)
                                os.makedirs(dataset_dir, exist_ok=True)

                                # ç§»åŠ¨æ–‡ä»¶
                                for item in os.listdir(temp_dir):
                                    s = os.path.join(temp_dir, item)
                                    d = os.path.join(dataset_dir, item)
                                    if os.path.isdir(s):
                                        shutil.copytree(s, d, dirs_exist_ok=True)
                                    else:
                                        shutil.copy2(s, d)

                                st.success(f"ğŸ‰ æ•°æ®é›† '{dataset_name}' å·²ä¿å­˜!")
                                st.session_state.selected_dataset = dataset_name

                                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                                shutil.rmtree(temp_dir)
            st.markdown('</div>', unsafe_allow_html=True)

    # # æ¨¡å‹è®­ç»ƒé¡µé¢
    elif page == "æ¨¡å‹è®­ç»ƒ":
        with st.container():
            st.markdown('<div class="dynamic-border">', unsafe_allow_html=True)
            st.header("âš™ï¸ æ¨¡å‹è®­ç»ƒé…ç½®")

            # é€‰æ‹©æ•°æ®é›†éƒ¨åˆ†
            dataset_dir = "datasets"
            if os.path.exists(dataset_dir) and len(os.listdir(dataset_dir)) > 0:
                datasets = os.listdir(dataset_dir)
                selected_dataset = st.selectbox(
                    "é€‰æ‹©æ•°æ®é›†",
                    datasets,
                    index=datasets.index(
                        st.session_state.selected_dataset) if st.session_state.selected_dataset in datasets else 0
                )

                # æ˜¾ç¤ºåŠ¨æ€æ•°æ®é›†ä¿¡æ¯
                with st.expander("æ•°æ®é›†ä¿¡æ¯", expanded=True):
                    dataset_path = os.path.join(dataset_dir, selected_dataset)
                    if os.path.exists(dataset_path):
                        train_images = len(os.listdir(os.path.join(dataset_path, "images", "train")))
                        val_images = len(os.listdir(os.path.join(dataset_path, "images", "val")))

                        col1, col2 = st.columns(2)
                        with col1:
                            st.metric("è®­ç»ƒå›¾åƒæ•°é‡", train_images)
                        with col2:
                            st.metric("éªŒè¯å›¾åƒæ•°é‡", val_images)
                    else:
                        st.warning("æ•°æ®é›†è·¯å¾„ä¸å­˜åœ¨")

                # åŠ¨æ€æ¨¡å‹é…ç½®éƒ¨åˆ†
                with st.expander("æ¨¡å‹é…ç½®", expanded=True):
                    col1, col2 = st.columns(2)

                    with col1:
                        model_size = st.selectbox("æ¨¡å‹å°ºå¯¸",
                                                  ["nano (n)", "small (s)", "medium (m)", "large (l)", "xlarge (x)"],
                                                  index=1)
                        epochs = st.slider("è®­ç»ƒè½®æ¬¡ (epochs)", min_value=1, max_value=500, value=100)
                        batch_size = st.selectbox("æ‰¹æ¬¡å¤§å° (batch size)", [4, 8, 16, 32, 64], index=1)

                    with col2:
                        img_size = st.selectbox("å›¾åƒå°ºå¯¸ (image size)", [320, 416, 512, 640], index=3)
                        learning_rate = st.slider("å­¦ä¹ ç‡ (learning rate)", min_value=0.0001, max_value=0.1, value=0.01,
                                                  step=0.001, format="%.3f")
                        patience = st.number_input("æ—©åœè€å¿ƒå€¼ (patience)", min_value=1, value=50)

                # åŠ¨æ€é«˜çº§é€‰é¡¹éƒ¨åˆ†
                with st.expander("é«˜çº§é€‰é¡¹"):
                    col3, col4 = st.columns(2)

                    with col3:
                        optimizer = st.selectbox("ä¼˜åŒ–å™¨", ["SGD", "Adam", "AdamW"], index=0)
                        weight_decay = st.number_input("æƒé‡è¡°å‡ (weight decay)", min_value=0.0, value=0.0005,
                                                       step=0.0001, format="%.4f")

                    with col4:
                        augment = st.checkbox("æ•°æ®å¢å¼º", value=True)
                        save_period = st.number_input("ä¿å­˜é—´éš” (save period)", min_value=1, value=10)

                # åŠ¨æ€è®­ç»ƒæ§åˆ¶éƒ¨åˆ†
                with st.expander("è®­ç»ƒæ§åˆ¶", expanded=True):
                    if st.button("ğŸš€ å¼€å§‹è®­ç»ƒ") and not st.session_state.training:
                        st.session_state.training = True

                        # åˆ›å»ºè¾“å‡ºç›®å½•
                        current_time = datetime.now().strftime("%Y%m%d_%H%M%S")
                        output_dir = os.path.join("runs", f"train_{current_time}")
                        os.makedirs(output_dir, exist_ok=True)

                        # æ˜¾ç¤ºåŠ¨æ€è®­ç»ƒä¿¡æ¯
                        st.info("è®­ç»ƒé…ç½®ä¿¡æ¯:")
                        col1, col2 = st.columns(2)
                        with col1:
                            st.write(f"**æ•°æ®é›†:** {selected_dataset}")
                            st.write(f"**æ¨¡å‹å°ºå¯¸:** {model_size}")
                            st.write(f"**è®­ç»ƒè½®æ¬¡:** {epochs}")
                        with col2:
                            st.write(f"**æ‰¹æ¬¡å¤§å°:** {batch_size}")
                            st.write(f"**å›¾åƒå°ºå¯¸:** {img_size}")
                            st.write(f"**å­¦ä¹ ç‡:** {learning_rate}")

                        # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
                        progress_bar = st.progress(0)
                        status_text = st.empty()

                        for i in range(1, 101):
                            progress_bar.progress(i)
                            status_text.text(f"è®­ç»ƒè¿›åº¦: {i}%")
                            # è¿™é‡Œåº”è¯¥æ˜¯å®é™…çš„è®­ç»ƒè¿‡ç¨‹

                            # time.sleep(0.1)  # æ¨¡æ‹Ÿè®­ç»ƒå»¶è¿Ÿ

                        st.session_state.training = False
                        st.success("ğŸ‰ è®­ç»ƒå®Œæˆ!")

                    elif st.session_state.training:
                        st.warning("è®­ç»ƒæ­£åœ¨è¿›è¡Œä¸­...")
                        if st.button("ğŸ›‘ åœæ­¢è®­ç»ƒ"):
                            st.session_state.training = False
                            st.experimental_rerun()
            else:
                st.warning("æ²¡æœ‰å¯ç”¨çš„æ•°æ®é›†ï¼Œè¯·å…ˆä¸Šä¼ æ•°æ®é›†")
            st.markdown('</div>', unsafe_allow_html=True)
    #
    # # è®­ç»ƒç›‘æ§é¡µé¢
    elif page == "è®­ç»ƒç›‘æ§":
        st.write("demo")
    #     with st.container():
    #         st.markdown('<div class="dynamic-border">', unsafe_allow_html=True)
    #         st.header("ğŸ“Š è®­ç»ƒç›‘æ§")
    #
    #         # æ£€æŸ¥æ˜¯å¦æœ‰è®­ç»ƒè¿è¡Œ
    #         runs_dir = "runs"
    #         if os.path.exists(runs_dir) and len(os.listdir(runs_dir)) > 0:
    #             runs = sorted(os.listdir(runs_dir), reverse=True)
    #             selected_run = st.selectbox("é€‰æ‹©è®­ç»ƒè¿è¡Œ", runs)
    #
    #             run_path = os.path.join(runs_dir, selected_run)
    #
    #             # åŠ¨æ€æ˜¾ç¤ºè®­ç»ƒç»“æœ
    #             with st.expander("è®­ç»ƒç»“æœ", expanded=True):
    #                 # æ¨¡æ‹Ÿç»“æœå›¾è¡¨
    #                 col1, col2 = st.columns(2)
    #                 with col1:
    #                     st.line_chart({"æŸå¤±": [0.8, 0.6, 0.4, 0.3, 0.25, 0.2]}, height=300)
    #                     st.caption("è®­ç»ƒæŸå¤±æ›²çº¿")
    #                 with col2:
    #                     st.line_chart({"å‡†ç¡®ç‡": [0.2, 0.4, 0.6, 0.7, 0.75, 0.8]}, height=300)
    #                     st.caption("éªŒè¯å‡†ç¡®ç‡æ›²çº¿")
    #
    #             # åŠ¨æ€æ˜¾ç¤ºè®­ç»ƒæ—¥å¿—
    #             with st.expander("è®­ç»ƒæ—¥å¿—"):
    #                 # æ¨¡æ‹Ÿæ—¥å¿—å†…å®¹
    #                 log_content = """
    # [2023-01-01 10:00:00] è®­ç»ƒå¼€å§‹: yolov8s on my_dataset
    # [2023-01-01 10:05:00] Epoch 1/100 - loss: 0.8 - accuracy: 0.2
    # [2023-01-01 10:10:00] Epoch 10/100 - loss: 0.6 - accuracy: 0.4
    # [2023-01-01 10:15:00] Epoch 20/100 - loss: 0.4 - accuracy: 0.6
    # [2023-01-01 10:20:00] Epoch 30/100 - loss: 0.3 - accuracy: 0.7
    # [2023-01-01 10:25:00] Epoch 40/100 - loss: 0.25 - accuracy: 0.75
    # [2023-01-01 10:30:00] Epoch 50/100 - loss: 0.2 - accuracy: 0.8
    # [2023-01-01 10:35:00] è®­ç»ƒå®Œæˆ - æœ€ä½³æ¨¡å‹ä¿å­˜åœ¨ runs/train_20230101_100000/weights/best.pt
    #                 """
    #                 st.text_area("æ—¥å¿—å†…å®¹", log_content, height=300)
    #         else:
    #             st.warning("æ²¡æœ‰å¯ç”¨çš„è®­ç»ƒè¿è¡Œ")
    #         st.markdown('</div>', unsafe_allow_html=True)

# ä¸»ç•Œé¢
def main():
## è¾¹æ¡†åŠŸèƒ½
    # Streamlit åº”ç”¨
    st.sidebar.markdown('<h1 class="sidebar-title">ğŸ§­ å¯¼èˆªèœå•</h1>', unsafe_allow_html=True)
    page = st.sidebar.radio("",
                        ["æ¨¡å‹é€‰æ‹©ï¼ˆå·²æä¾›ï¼‰", "æ¨¡å‹è‡ªå®šä¹‰ï¼ˆè®­ç»ƒï¼‰"],
                        index=0,
                        format_func=lambda x: "ğŸ” å¿«é€Ÿæ£€æµ‹" if x == "æ¨¡å‹é€‰æ‹©ï¼ˆå·²æä¾›ï¼‰" else "ğŸ› ï¸ æ¨¡å‹è®­ç»ƒ"
                        )

    if page == "æ¨¡å‹é€‰æ‹©ï¼ˆå·²æä¾›ï¼‰":
        model_usage()
    else:
        model_train()


# è¿è¡Œ Streamlit åº”ç”¨
if __name__ == "__main__":
    main()
