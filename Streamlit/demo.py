import csv
from datetime import datetime
import zipfile
import shutil
import numpy as np
import pandas as pd
from ultralytics import YOLO
from PIL import Image
from io import BytesIO
import os
import threading
import time
import cv2
import streamlit as st
from util import convert_video_with_ffmpeg
import plotly.express as px

# ======================
# è‡ªå®šä¹‰CSSæ ·å¼
# ======================
st.markdown("""
<style>
    /* ä¸»å®¹å™¨æ ·å¼ */
    .main-container {
        background: linear-gradient(145deg, #f8f9fa 0%, #e9ecef 100%);
        border-radius: 15px;
        padding: 2rem;
        box-shadow: 0 4px 20px rgba(0,0,0,0.1);
        margin: 1rem 0;
    }

    /* åŠ¨æ€è¾¹æ¡†åŠ¨ç”» */
    @keyframes border-glow {
        0% { box-shadow: 0 0 10px rgba(76,175,80,0.3); }
        50% { box-shadow: 0 0 15px rgba(33,150,243,0.5); }
        100% { box-shadow: 0 0 10px rgba(76,175,80,0.3); }
    }

    .section-card {
        border: 1px solid #dee2e6;
        border-radius: 12px;
        padding: 1.5rem;
        margin: 1rem 0;
        background: white;
        animation: border-glow 3s ease-in-out infinite;
    }

    /* æŒ‰é’®ç¾åŒ– */
    .stButton>button {
        background: linear-gradient(45deg, #4CAF50, #2196F3) !important;
        color: white !important;
        border: none !important;
        border-radius: 25px !important;
        padding: 0.8rem 1.5rem !important;
        font-weight: 600 !important;
        transition: all 0.3s !important;
    }

    .stButton>button:hover {
        transform: scale(1.05);
        box-shadow: 0 4px 15px rgba(0,0,0,0.2) !important;
    }

    /* ä¾§è¾¹æ ç¾åŒ– */
    [data-testid="stSidebar"] {
        background: linear-gradient(180deg, #2c3e50, #34495e) !important;
        box-shadow: 2px 0 15px rgba(0,0,0,0.1);
    }

    .sidebar-title {
        color: #fff !important;
        font-size: 1.5rem !important;
        padding: 1rem 0;
        text-align: center;
    }
</style>
""", unsafe_allow_html=True)

# åˆå§‹åŒ–å…¨å±€å˜é‡
MODEL_PATHS = {
    "YOLOv11":"D:\Python\graduate_design\Model\yolo11n.pt",
    "YOLO_detect_animals":r"D:\Python\graduate_design\Model\runs\train\exp\weights\best.pt", #
}

current_model = YOLO(r"D:\Python\graduate_design\Model\yolo11n.pt")# é»˜è®¤
conf_threshold = 0
iou_threshold = 0
detection_data = []
detections_df = None
speed_df = None
flag = False # ç”¨äºæ£€æµ‹æœ‰æ— å›¾åƒ
target_class = []


def process_yolo_results(results, class_list=None, conf_thres=0.1):
    global flag
    # """
    # å¤„ç†YOLOç»“æœå¹¶è¿”å›å¯ç›´æ¥æ˜¾ç¤ºçš„å›¾åƒ
    # :param results: YOLOæ£€æµ‹ç»“æœ(å•ä¸ªResultså¯¹è±¡)
    # :param class_list: è¦æ˜¾ç¤ºçš„ç±»åˆ«åˆ—è¡¨
    # :param conf_thres: ç½®ä¿¡åº¦é˜ˆå€¼
    # :return: å¯ç›´æ¥æ˜¾ç¤ºçš„numpyæ•°ç»„å›¾åƒ(BGRæ ¼å¼)
    # """
    # 1. ç»“æœè¿‡æ»¤
    if class_list is not None:
        names = results.names
        keep_idx = [
            i for i, box in enumerate(results.boxes)
            if (names[int(box.cls)] in class_list) and (float(box.conf) >= conf_thres)
        ]
        results.boxes = results.boxes[keep_idx]
        if hasattr(results, 'masks') and results.masks is not None:
            results.masks = results.masks[keep_idx]
        if hasattr(results, 'keypoints') and results.keypoints is not None:
            results.keypoints = results.keypoints[keep_idx]

    if len(results.boxes)==0 :
        flag = False
    # 2. å®‰å…¨å›¾åƒè½¬æ¢
    plotted_img = results.plot()

    # å¤„ç†ä¸åŒè¿”å›ç±»å‹
    if isinstance(plotted_img, Image.Image):
        # PIL.Imageè½¬numpyæ•°ç»„
        img_np = np.array(plotted_img)
        # ç¡®ä¿æ˜¯3é€šé“(RGBæˆ–BGR)
        if img_np.ndim == 2:  # ç°åº¦å›¾
            img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        elif img_np.shape[2] == 4:  # RGBA
            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR)
        else:  # RGB
            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGB2BGR)
    else:  # å·²ç»æ˜¯numpyæ•°ç»„
        img_np = plotted_img
        if img_np.ndim == 2:  # ç°åº¦å›¾
            img_np = cv2.cvtColor(img_np, cv2.COLOR_GRAY2BGR)
        elif img_np.shape[2] == 4:  # RGBA
            img_np = cv2.cvtColor(img_np, cv2.COLOR_RGBA2BGR)
        elif img_np.shape[2] == 3:  # ç¡®ä¿æ˜¯BGR
            pass  # å‡è®¾å·²ç»æ˜¯BGR

    return img_np

# åŠ è½½æ¨¡å‹
def load_model(model_name):
    global current_model
    model_path = MODEL_PATHS.get(model_name)
    if not model_path:
        st.error("æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼")
        return None
    try:
        model = YOLO(model_path)
        st.success(f"æˆåŠŸåŠ è½½æ¨¡å‹ï¼š{model_name}")
        return model
    except Exception as e:
        st.error(f"åŠ è½½æ¨¡å‹å¤±è´¥ï¼š{e}")
        return None

def yolo_results_to_dataframe(results):
    """
    å°†YOLOæ£€æµ‹ç»“æœè½¬æ¢ä¸ºç»“æ„åŒ–DataFrame
    """
    global detection_data
    result = results[0]

    try:
        for i, box in enumerate(result.boxes, 1):
            detection_data.append({
            "åºå·": i,
            "ç±»åˆ«ID": int(box.cls.item()),
            "ç±»åˆ«åç§°": result.names[int(box.cls.item())],
            "ç½®ä¿¡åº¦": float(box.conf.item()),
            "x1": round(box.xyxy[0][0].item()),
            "y1": round(box.xyxy[0][1].item()),
            "x2": round(box.xyxy[0][2].item()),
            "y2": round(box.xyxy[0][3].item()),
            "å®½åº¦": round(box.xyxy[0][2].item() - box.xyxy[0][0].item()),
            "é«˜åº¦": round(box.xyxy[0][3].item() - box.xyxy[0][1].item())
            })
    except Exception as e:
            st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
            return

    return pd.DataFrame(detection_data), pd.DataFrame([result.speed])
# å›¾ç‰‡æ£€æµ‹
def image_detection():
    global current_model
    global detection_data
    global detections_df
    global speed_df
    global flag
    global iou_threshold
    global conf_threshold

    st.header("å›¾ç‰‡æ£€æµ‹")

    # ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶
    uploaded_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡æ–‡ä»¶",
                                   type=["jpg", "jpeg", "png"],
                                   label_visibility="visible")

    if uploaded_file:
        flag = True
        # åˆ›å»ºä¸¤åˆ—å¸ƒå±€
        col1, col2 = st.columns(2)

        # æ‰“å¼€åŸå§‹å›¾åƒ
        original_image = Image.open(uploaded_file)
        img_array = np.array(original_image)
        img_cv = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)

        # å·¦ä¾§æ˜¾ç¤ºåŸå§‹å›¾ç‰‡
        with col1:
            st.subheader("åŸå§‹å›¾ç‰‡")
            st.image(original_image,  use_container_width=True)

        # å³ä¾§æ˜¾ç¤ºå¤„ç†è¿›åº¦å’Œç»“æœ
        with col2:
            st.subheader("æ£€æµ‹ç»“æœ")

            with st.spinner("YOLOæ¨¡å‹æ­£åœ¨å¤„ç†ä¸­..."):
                try:
                    # ä½¿ç”¨YOLOæ¨¡å‹è¿›è¡Œæ¨ç†
                    results = current_model(
                        source=img_cv,
                        conf=conf_threshold,
                        iou=iou_threshold,
                        save=False
                    )

                    process_yolo_results(results[0],class_list=target_class,
                                                    )
                    annotated_image = results[0].plot()
                    annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB)
                    st.image(annotated_image, use_container_width=True)

                    # è½¬æ¢ç»“æœä¸ºDataFrame
                    if hasattr(results[0], 'boxes'):
                        detections_df, speed_df = yolo_results_to_dataframe(results)

                    # æ·»åŠ ä¸‹è½½åŠŸèƒ½
                    buffered = BytesIO()
                    Image.fromarray(annotated_image).save(buffered, format="JPEG")
                    st.download_button(
                        label="ğŸ“¥ å¯¼å‡ºæ£€æµ‹å›¾ç‰‡",
                        data=buffered.getvalue(),
                        file_name=f"detection_{datetime.now().strftime('%Y%m%d%H%M%S')}.jpg",
                        mime="image/jpeg",
                        type="primary"
                    )

                except Exception as e:
                    st.error(f"å¤„ç†å¤±è´¥: {str(e)}")
                    return

        # ä¸‹æ–¹æ˜¾ç¤ºè¯¦ç»†æ•°æ®
        st.divider()
        st.subheader("æ£€æµ‹æ•°æ®è¯¦æƒ…")

        if flag :
            # æ ¼å¼åŒ–è¡¨æ ¼
            display_df = detections_df.copy()
            display_df['ç½®ä¿¡åº¦'] = display_df['ç½®ä¿¡åº¦'].apply(lambda x: f"{x:.2%}")

            # äº¤äº’å¼è¡¨æ ¼
            st.dataframe(
                display_df[['åºå·', 'ç±»åˆ«åç§°', 'ç½®ä¿¡åº¦', 'x1', 'y1', 'x2', 'y2']],
                column_config={
                    "ç½®ä¿¡åº¦": st.column_config.ProgressColumn(
                        min_value=0,
                        max_value=1,
                        format="%.2f%%"
                    )
                },
                hide_index=True,
                use_container_width=True
            )

            # ç»Ÿè®¡ä¿¡æ¯
            col_stat1, col_stat2 = st.columns(2)
            with col_stat1:
                st.metric("æ£€æµ‹ç›®æ ‡æ€»æ•°", len(detections_df))
            with col_stat2:
                st.metric("å¹³å‡ç½®ä¿¡åº¦", f"{detections_df['ç½®ä¿¡åº¦'].mean():.2%}")

            # ç±»åˆ«åˆ†å¸ƒ
            st.subheader("ç±»åˆ«åˆ†å¸ƒ")
            st.bar_chart(detections_df['ç±»åˆ«åç§°'].value_counts())

            # å¤„ç†é€Ÿåº¦å’Œæ•°æ®ä¸‹è½½
            with st.expander("é«˜çº§é€‰é¡¹"):
                tab1, tab2 = st.tabs(["æ€§èƒ½æŒ‡æ ‡", "æ•°æ®å¯¼å‡º"])
                with tab1:
                    st.dataframe(
                        speed_df.rename(columns={
                            'preprocess': 'é¢„å¤„ç†(ms)',
                            'inference': 'æ¨ç†(ms)',
                            'postprocess': 'åå¤„ç†(ms)'
                        }),
                        use_container_width=True
                    )
                with tab2:
                    st.download_button(
                        label="ğŸ“Š å¯¼å‡ºæ£€æµ‹æ•°æ®(CSV)",
                        data=detections_df.to_csv(index=False).encode('utf-8'),
                        file_name="detection_data.csv",
                        mime="text/csv"
                    )

        # æ§åˆ¶å°è¾“å‡ºï¼ˆè°ƒè¯•ç”¨ï¼‰
        #     print("==== æ£€æµ‹ç»“æœ ====")
        #     print(detections_df[['åºå·', 'ç±»åˆ«åç§°', 'ç½®ä¿¡åº¦', 'x1', 'y1', 'x2', 'y2']].to_string(index=False))
        #     print("\n==== å¤„ç†é€Ÿåº¦ (ms) ====")
        #     print(speed_df.to_string(index=False))



def video_detection():
    global current_model
    global detection_data
    global conf_threshold
    global iou_threshold
    st.header("ğŸ¥ è§†é¢‘æ£€æµ‹ç³»ç»Ÿ")

    # # åœ¨ä¾§è¾¹æ æ·»åŠ ç±»åˆ«é€‰æ‹©åŠŸèƒ½
    # with st.sidebar.expander("ğŸ” æ£€æµ‹ç±»åˆ«è®¾ç½®", expanded=True):
    #     # è·å–æ¨¡å‹æ”¯æŒçš„ç±»åˆ«åˆ—è¡¨
    #     class_options = list(current_model.names.values())
    #     selected_classes = st.multiselect(
    #         "é€‰æ‹©è¦æ£€æµ‹çš„ç±»åˆ«",
    #         options=class_options,
    #         default=class_options[:3],  # é»˜è®¤é€‰æ‹©å‰3ä¸ªç±»åˆ«
    #         help="é€‰æ‹©éœ€è¦æ£€æµ‹çš„ç›®æ ‡ç±»åˆ«"
    #     )

    # ä½¿ç”¨å®¹å™¨åˆ’åˆ†ä¸»è¦åŠŸèƒ½åŒºåŸŸ
    upload_container = st.container(border=True)
    with upload_container:
        # ä¸Šä¼ åŒºåŸŸ
        st.subheader("1. è§†é¢‘ä¸Šä¼ ")
        uploaded_file = st.file_uploader("é€‰æ‹©å¾…æ£€æµ‹è§†é¢‘æ–‡ä»¶",
                                         type=["mp4", "avi", "mov"],
                                         label_visibility="collapsed",
                                         help="æ”¯æŒ MP4/AVI/MOV æ ¼å¼ï¼Œæœ€å¤§æ–‡ä»¶å¤§å° 500MB")

    if uploaded_file:
        # ä¿å­˜ä¸Šä¼ çš„è§†é¢‘åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_video_path = "temp_uploaded_video.mp4"
        with open(temp_video_path, "wb") as f:
            f.write(uploaded_file.getbuffer())

        # è§†é¢‘é¢„è§ˆå’Œå¤„ç†åŒºåŸŸ
        process_container = st.container(border=True)
        with process_container:
            st.subheader("2. è§†é¢‘å¤„ç†")

            # åˆ›å»ºå¯¹æ¯”å¸ƒå±€
            col1, col2 = st.columns([0.45, 0.55], gap="large")

            with col1:
                # åŸå§‹è§†é¢‘é¢„è§ˆ
                st.markdown("**åŸå§‹è§†é¢‘é¢„è§ˆ**")
                st.video(temp_video_path)

            with col2:
                # å¤„ç†çŠ¶æ€åŒºåŸŸ
                status_col1, status_col2 = st.columns([0.7, 0.3])
                with status_col1:
                    st.markdown("**å¤„ç†çŠ¶æ€**")
                    progress_placeholder = st.empty()

        # å¤„ç†ç»“æœåŒºåŸŸ
        processed_video_placeholder = st.empty()
        export_placeholder = st.empty()
        visualization_placeholder = st.empty()

        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = os.path.join("temp_results", "processed_video")
        os.makedirs(output_dir, exist_ok=True)
        output_video_name = "output_" + os.path.basename(temp_video_path)
        processed_temp_video_path = os.path.join(output_dir, "temp_processed_video.avi")
        final_processed_video_path = os.path.join(output_dir, output_video_name)
        detection_csv_path = os.path.join(output_dir, "detection_results.csv")

        try:
            # è·å–è§†é¢‘ä¿¡æ¯
            cap = cv2.VideoCapture(temp_video_path)
            if not cap.isOpened():
                raise ValueError("æ— æ³•æ‰“å¼€ä¸Šä¼ çš„è§†é¢‘æ–‡ä»¶")
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            fps = cap.get(cv2.CAP_PROP_FPS)
            width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
            height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
            cap.release()

            # åˆå§‹åŒ–æ£€æµ‹ç»“æœæ”¶é›†
            frame_timestamps = []
            lock = threading.Lock()

            def process_video():
                nonlocal processed_frames
                try:
                    # åˆå§‹åŒ–CSVæ–‡ä»¶
                    with open(detection_csv_path, mode='w', newline='') as csv_file:
                        fieldnames = ['frame_num', 'timestamp', 'class', 'confidence', 'x1', 'y1', 'x2', 'y2']
                        writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
                        writer.writeheader()

                    # ä½¿ç”¨YOLOè¿›è¡Œè§†é¢‘æ£€æµ‹
                    results = current_model.track(
                        source=temp_video_path,
                        conf=conf_threshold,
                        iou=iou_threshold,
                        stream=True,
                        imgsz=(width, height),
                        save=False,
                        project=None,
                        name=None,
                        verbose=False
                    )

                    # æ‰‹åŠ¨ä¿å­˜å¤„ç†åçš„è§†é¢‘
                    fourcc = cv2.VideoWriter_fourcc(*'XVID')
                    out = cv2.VideoWriter(processed_temp_video_path, fourcc, fps, (width, height))

                    for frame_idx, frame_result in enumerate(results):
                        # è·å–å½“å‰å¸§æ—¶é—´æˆ³ï¼ˆç§’ï¼‰
                        current_time = frame_idx / fps

                        # ç­›é€‰æŒ‡å®šç±»åˆ«çš„æ£€æµ‹ç»“æœ
                        if target_class:  # å¦‚æœç”¨æˆ·é€‰æ‹©äº†ç‰¹å®šç±»åˆ«
                            keep_idx = [
                                i for i, box in enumerate(frame_result.boxes)
                                if current_model.names[int(box.cls)] in target_class
                            ]
                            frame_result.boxes = frame_result.boxes[keep_idx]
                            if hasattr(frame_result, 'masks') and frame_result.masks is not None:
                                frame_result.masks = frame_result.masks[keep_idx]
                            if hasattr(frame_result, 'keypoints') and frame_result.keypoints is not None:
                                frame_result.keypoints = frame_result.keypoints[keep_idx]

                        # ç»˜åˆ¶æ£€æµ‹ç»“æœåˆ°å¸§
                        processed_frame = frame_result.plot()
                        out.write(processed_frame)

                        # æ”¶é›†æ£€æµ‹æ•°æ®ï¼ˆåªè®°å½•é€‰ä¸­çš„ç±»åˆ«ï¼‰
                        for detection in frame_result.boxes:
                            class_id = int(detection.cls)
                            class_name = current_model.names[class_id]
                            conf = float(detection.conf)
                            bbox = detection.xyxy[0].tolist()

                            # çº¿ç¨‹å®‰å…¨åœ°æ›´æ–°æ•°æ®
                            with lock:
                                detection_data.append({
                                    'frame_num': frame_idx,
                                    'timestamp': current_time,
                                    'class': class_name,
                                    'confidence': conf,
                                    'x1': bbox[0],
                                    'y1': bbox[1],
                                    'x2': bbox[2],
                                    'y2': bbox[3]
                                })
                                processed_frames = frame_idx + 1
                                frame_timestamps.append(current_time)

                        # æ¯å¤„ç†10å¸§æˆ–ç»“æŸæ—¶ä¿å­˜ä¸€æ¬¡CSV
                        if frame_idx % 10 == 0 or frame_idx == total_frames - 1:
                            with lock:
                                with open(detection_csv_path, mode='a', newline='') as csv_file:
                                    writer = csv.DictWriter(csv_file, fieldnames=fieldnames)
                                    writer.writerows(detection_data[-10:])

                    out.release()
                except Exception as e:
                    st.error(f"è§†é¢‘å¤„ç†å†…éƒ¨é”™è¯¯: {str(e)}")

            # å¯åŠ¨å¤„ç†çº¿ç¨‹
            processed_frames = 0
            process_thread = threading.Thread(target=process_video)
            process_thread.start()

            # å®æ—¶æ›´æ–°è¿›åº¦
            while process_thread.is_alive():
                with lock:
                    progress = min(100, int((processed_frames / total_frames) * 100) if total_frames > 0 else 0)
                progress_placeholder.progress(
                    progress,
                    text=f"â– å¤„ç†è¿›åº¦: {progress}% | å·²å¤„ç† {processed_frames}/{total_frames} å¸§"
                )
                time.sleep(0.1)

            process_thread.join()

            # è§†é¢‘æ ¼å¼è½¬æ¢
            if os.path.exists(processed_temp_video_path):
                convert_video_with_ffmpeg(processed_temp_video_path, final_processed_video_path)

            # æ˜¾ç¤ºå¤„ç†ç»“æœ
            with process_container:
                with col2:
                    if os.path.exists(final_processed_video_path):
                        processed_video_placeholder.markdown("**æ£€æµ‹ç»“æœé¢„è§ˆ**")
                        processed_video_placeholder.video(final_processed_video_path)

                        # å¯¼å‡ºæŒ‰é’®
                        with export_placeholder:
                            col_export1, col_export2 = st.columns(2)
                            with col_export1:
                                with open(final_processed_video_path, "rb") as f:
                                    st.download_button(
                                        label="â¬‡ï¸ å¯¼å‡ºç»“æœè§†é¢‘",
                                        data=f,
                                        file_name=output_video_name,
                                        mime="video/mp4",
                                        use_container_width=True,
                                        type="primary"
                                    )
                            with col_export2:
                                with open(detection_csv_path, "rb") as f:
                                    st.download_button(
                                        label="ğŸ“Š å¯¼å‡ºæ£€æµ‹æ•°æ®(CSV)",
                                        data=f,
                                        file_name="detection_results.csv",
                                        mime="text/csv",
                                        use_container_width=True
                                    )

            # æ•°æ®å¯è§†åŒ–ï¼ˆåªæ˜¾ç¤ºé€‰ä¸­çš„ç±»åˆ«ï¼‰
            if os.path.exists(detection_csv_path):
                with visualization_placeholder.container(border=True):
                    st.subheader("3. æ£€æµ‹æ•°æ®åˆ†æ")

                    # åŠ è½½æ£€æµ‹æ•°æ®
                    df = pd.read_csv(detection_csv_path)

                    # ç­›é€‰é€‰ä¸­çš„ç±»åˆ«
                    if target_class:
                        df = df[df['class'].isin(target_class)]

                    if not df.empty:
                        # åˆ›å»ºå¯è§†åŒ–é€‰é¡¹å¡
                        tab1, tab2, tab3 = st.tabs(["ç±»åˆ«åˆ†å¸ƒ", "æ—¶é—´è¶‹åŠ¿", "ç©ºé—´åˆ†å¸ƒ"])

                        with tab1:
                            st.markdown("**æ£€æµ‹ç±»åˆ«ç»Ÿè®¡**")
                            species_counts = df['class'].value_counts().reset_index()
                            species_counts.columns = ['Class', 'Count']
                            fig1 = px.bar(species_counts,
                                          x='Class',
                                          y='Count',
                                          color='Class',
                                          text='Count')
                            st.plotly_chart(fig1, use_container_width=True)

                        with tab2:
                            st.markdown("**æ£€æµ‹ç»“æœæ—¶é—´åˆ†å¸ƒ**")
                            df['time_interval'] = (df['timestamp'] // 5) * 5  # 5ç§’é—´éš”åˆ†ç»„
                            time_dist = df.groupby(['time_interval', 'class']).size().reset_index(name='count')
                            fig2 = px.line(time_dist,
                                           x='time_interval',
                                           y='count',
                                           color='class',
                                           markers=True)
                            fig2.update_xaxes(title="æ—¶é—´ (ç§’)")
                            fig2.update_yaxes(title="æ£€æµ‹æ•°é‡")
                            st.plotly_chart(fig2, use_container_width=True)

                        with tab3:
                            st.markdown("**æ£€æµ‹ç›®æ ‡ç©ºé—´åˆ†å¸ƒ**")
                            fig3 = px.scatter(df,
                                              x='x1',
                                              y='y1',
                                              color='class',
                                              size='confidence',
                                              hover_data=['frame_num', 'confidence'])
                            fig3.update_xaxes(range=[0, width])
                            fig3.update_yaxes(range=[height, 0])  # åè½¬Yè½´åŒ¹é…å›¾åƒåæ ‡
                            st.plotly_chart(fig3, use_container_width=True)
                    else:
                        st.warning("æœªæ£€æµ‹åˆ°é€‰å®šç±»åˆ«çš„ç›®æ ‡")

        except Exception as e:
            st.error(f"âŒ å¤„ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}", icon="ğŸš¨")

        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if os.path.exists(temp_video_path):
                os.remove(temp_video_path)
            if os.path.exists(processed_temp_video_path):
                os.remove(processed_temp_video_path)


def real_time_detection():
    """å®æ—¶ç›®æ ‡æ£€æµ‹å‡½æ•°"""
    st.title("å®æ—¶ç›®æ ‡æ£€æµ‹")
    global current_model
    global conf_threshold
    global iou_threshold


    start_button = st.button("å¼€å§‹æ£€æµ‹")
    stop_button = st.button("åœæ­¢æ£€æµ‹")

    if start_button:
        cap = cv2.VideoCapture(0)
        if not cap.isOpened():
            st.error("æ— æ³•æ‰“å¼€è§†é¢‘æº")
            return

        st_frame = st.empty()  # ç”¨äºåŠ¨æ€æ›´æ–°ç”»é¢çš„å ä½ç¬¦
        stats_placeholder = st.empty()  # ç»Ÿè®¡ä¿¡æ¯å ä½ç¬¦

        while cap.isOpened() and not stop_button:
            success, frame = cap.read()
            if not success:
                st.warning("è§†é¢‘æµç»“æŸ")
                break

            # æ‰§è¡Œæ£€æµ‹
            start_time = time.time()
            results = current_model(frame,
                            conf=conf_threshold,
                            iou=iou_threshold,
                            verbose=False)

            # è®¡ç®—FPS
            fps = 1 / (time.time() - start_time + 1e-9)

            # ç»˜åˆ¶ç»“æœ
            annotated_frame = results[0].plot()
            annotated_frame = cv2.cvtColor(annotated_frame, cv2.COLOR_BGR2RGB)

            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
            num_objects = len(results[0].boxes)
            stats_text = f"""
            **æ£€æµ‹ç»Ÿè®¡**  
            â€¢ ç›®æ ‡æ•°é‡: {num_objects}  
            â€¢ ç½®ä¿¡åº¦é˜ˆå€¼: {conf_threshold:.2f}  
            â€¢ å®æ—¶FPS: {fps:.1f}  
            """

            # æ›´æ–°ç•Œé¢
            st_frame.image(annotated_frame, caption="å®æ—¶æ£€æµ‹ç”»é¢", use_container_width=True)
            stats_placeholder.markdown(stats_text)

            # æ§åˆ¶å¸§ç‡ (é»˜è®¤30FPS)
            time.sleep(1 / 30)

        cap.release()
        cv2.destroyAllWindows()

def model_usage():
    global current_model
    global iou_threshold
    global conf_threshold

    st.title("ğŸ¦ åŸºäºYOLOçš„åŠ¨ç‰©è¯†åˆ«ç³»ç»Ÿ")
    st.markdown("---")

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.markdown('<h1 class="sidebar-title">âš™ï¸ æ§åˆ¶é¢æ¿</h1>', unsafe_allow_html=True)

        # æ¨¡å‹é€‰æ‹©
        model_name = st.selectbox(
            "**é€‰æ‹©æ£€æµ‹æ¨¡å‹**",
            list(MODEL_PATHS.keys()),
            index=0,
            help="é€‰æ‹©é¢„è®­ç»ƒçš„YOLOæ¨¡å‹"
        )
        current_model = load_model(model_name)

        # å‚æ•°è®¾ç½®
        st.markdown("---")
        st.markdown("**âš–ï¸ æ£€æµ‹å‚æ•°**")
        conf_threshold = st.slider("ç½®ä¿¡åº¦é˜ˆå€¼", 0.0, 1.0, 0.25, step=0.01)
        iou_threshold = st.slider("IoU é˜ˆå€¼", 0.0, 1.0, 0.5, step=0.01)

        # è®¾å¤‡è®¾ç½®
        st.markdown("---")
        st.markdown("**ğŸ“· è¾“å…¥æºè®¾ç½®**")
        camera_devices = ["é»˜è®¤æ‘„åƒå¤´", "å¤–éƒ¨è®¾å¤‡1", "å¤–éƒ¨è®¾å¤‡2"]
        selected_camera = st.selectbox("è§†é¢‘è¾“å…¥æº", camera_devices)

        # åŠŸèƒ½å¯¼èˆª
        st.markdown("---")
        st.markdown("**ğŸ¯ æ£€æµ‹æ¨¡å¼**")
        task = st.radio("", ["å›¾ç‰‡æ£€æµ‹", "è§†é¢‘æ£€æµ‹", "å®æ—¶æ£€æµ‹"], index=0)

    # ä¸»å†…å®¹åŒºåŸŸ
    with st.container():

        if task == "å›¾ç‰‡æ£€æµ‹":
            image_detection()

        elif task == "è§†é¢‘æ£€æµ‹":
            video_detection()

        elif task == "å®æ—¶æ£€æµ‹":
            real_time_detection()

import yaml

# é…ç½®å¸¸é‡
TEMP_DIR = "temp_uploads"
DATA_YAML = os.path.join(TEMP_DIR, "data.yaml")
MODEL_YAML = os.path.join(TEMP_DIR, "model.yaml")


def setup_temp_dir():
    """è®¾ç½®ä¸´æ—¶ç›®å½•"""
    os.makedirs(TEMP_DIR, exist_ok=True)
    return TEMP_DIR


def cleanup_temp_dir():
    """æ¸…ç†ä¸´æ—¶ç›®å½•"""
    if os.path.exists(TEMP_DIR):
        shutil.rmtree(TEMP_DIR)


def save_uploaded_file(uploaded_file, save_path):
    """ä¿å­˜å•ä¸ªä¸Šä¼ çš„æ–‡ä»¶"""
    with open(save_path, "wb") as f:
        f.write(uploaded_file.getbuffer())
    return save_path


def create_data_yaml(train_dir, val_dir, class_names):
    """åˆ›å»ºdata.yamlé…ç½®æ–‡ä»¶"""
    data = {
        'train': train_dir,
        'val': val_dir,
        'nc': len(class_names),
        'names': class_names
    }

    with open(DATA_YAML, 'w') as f:
        yaml.dump(data, f)

    return DATA_YAML



def run_yolo_training(params):
    """æ‰§è¡ŒYOLOè®­ç»ƒå‘½ä»¤"""
    cmd = [
        'yolo',
        'task=detect',
        'mode=train',
        f'model={params["model_config"]}',
        f'data={params["data_config"]}',
        f'imgsz={params["imgsz"]}',
        f'epochs={params["epochs"]}',
        f'batch={params["batch"]}',
        f'workers={params["workers"]}',
        f'device={params["device"]}',
        f'optimizer={params["optimizer"]}',
        f'project={params["project"]}',
        f'name={params["name"]}',
    ]

    # æ·»åŠ å¸ƒå°”å‚æ•°
    if params["cache"]:
        cmd.append('cache=True')
    else:
        cmd.append('cache=False')

    if params["single_cls"]:
        cmd.append('single_cls=True')
    else:
        cmd.append('single_cls=False')

    if params["amp"]:
        cmd.append('amp=True')
    else:
        cmd.append('amp=False')

    if params["close_mosaic"] > 0:
        cmd.append(f'close_mosaic={params["close_mosaic"]}')

    # æ‰§è¡Œå‘½ä»¤
    st.info("å¼€å§‹è®­ç»ƒ...")
    st.code(" ".join(cmd))

    # è¿™é‡Œå®é™…ä¸Šåº”è¯¥ä½¿ç”¨subprocessæˆ–å…¶ä»–æ–¹å¼è¿è¡Œè®­ç»ƒ
    # ä¾‹å¦‚: subprocess.run(cmd, check=True)
    # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬åªæ˜¯æ˜¾ç¤ºå‘½ä»¤

    # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
    with st.spinner("è®­ç»ƒè¿›è¡Œä¸­..."):
        import time
        progress_bar = st.progress(0)
        for i in range(100):
            time.sleep(0.1)
            progress_bar.progress(i + 1)

    st.success("è®­ç»ƒå®Œæˆ!")


def model_train():
    # é¡µé¢è®¾ç½®
    """ä¸»å‡½æ•°"""
    st.title("YOLO æ¨¡å‹è®­ç»ƒé…ç½®")

    # åˆå§‹åŒ–ä¸´æ—¶ç›®å½•
    setup_temp_dir()

    # åˆ›å»ºè¡¨å•
    with st.form("train_form"):
        # 1. ä¸Šä¼ æ¨¡å‹é…ç½®æ–‡ä»¶
        st.subheader("1. ä¸Šä¼ æ¨¡å‹é…ç½®æ–‡ä»¶")
        model_file = st.file_uploader(
            "ä¸Šä¼ YOLOæ¨¡å‹é…ç½®æ–‡ä»¶ (.yaml)",
            type=['yaml'],
            help="ä¸Šä¼ YOLOæ¨¡å‹ç»“æ„é…ç½®æ–‡ä»¶"
        )

        # 2. ä¸Šä¼ è®­ç»ƒæ•°æ®
        st.subheader("2. ä¸Šä¼ è®­ç»ƒæ•°æ®")
        col1, col2 = st.columns(2)

        with col1:
            train_images = st.file_uploader(
                "ä¸Šä¼ è®­ç»ƒå›¾ç‰‡",
                type=['jpg', 'jpeg', 'png'],
                accept_multiple_files=True,
                help="ä¸Šä¼ è®­ç»ƒé›†å›¾ç‰‡æ–‡ä»¶"
            )

            train_labels = st.file_uploader(
                "ä¸Šä¼ è®­ç»ƒæ ‡æ³¨",
                type=['txt'],
                accept_multiple_files=True,
                help="ä¸Šä¼ è®­ç»ƒé›†æ ‡æ³¨æ–‡ä»¶ (YOLOæ ¼å¼)"
            )

        with col2:
            val_images = st.file_uploader(
                "ä¸Šä¼ éªŒè¯å›¾ç‰‡",
                type=['jpg', 'jpeg', 'png'],
                accept_multiple_files=True,
                help="ä¸Šä¼ éªŒè¯é›†å›¾ç‰‡æ–‡ä»¶"
            )

            val_labels = st.file_uploader(
                "ä¸Šä¼ éªŒè¯æ ‡æ³¨",
                type=['txt'],
                accept_multiple_files=True,
                help="ä¸Šä¼ éªŒè¯é›†æ ‡æ³¨æ–‡ä»¶ (YOLOæ ¼å¼)"
            )

        # 3. é…ç½®æ•°æ®é›†ä¿¡æ¯
        st.subheader("3. é…ç½®æ•°æ®é›†ä¿¡æ¯")
        class_names = st.text_area(
            "ç±»åˆ«åç§° (æ¯è¡Œä¸€ä¸ª)",
            value="class1\nclass2\nclass3",
            help="è¾“å…¥æ‰€æœ‰ç±»åˆ«åç§°ï¼Œæ¯è¡Œä¸€ä¸ª"
        )

        # 4. è®­ç»ƒå‚æ•°é…ç½®
        st.subheader("4. è®­ç»ƒå‚æ•°é…ç½®")
        col1, col2 = st.columns(2)

        with col1:
            epochs = st.number_input(
                "è®­ç»ƒè½®æ•° (epochs)",
                min_value=1,
                max_value=1000,
                value=100,
                step=1
            )

            batch = st.number_input(
                "æ‰¹é‡å¤§å° (batch)",
                min_value=1,
                max_value=64,
                value=8,
                step=1
            )

            imgsz = st.number_input(
                "å›¾åƒå¤§å° (imgsz)",
                min_value=32,
                max_value=1280,
                value=640,
                step=32
            )

        with col2:
            workers = st.number_input(
                "å·¥ä½œçº¿ç¨‹æ•° (workers)",
                min_value=0,
                max_value=16,
                value=0,
                step=1
            )

            device = st.text_input(
                "è®¾å¤‡ (device)",
                value="0",
                help="ä½¿ç”¨çš„GPUè®¾å¤‡IDï¼Œå¦‚'0'æˆ–'0,1,2'ï¼ŒCPUä½¿ç”¨'cpu'"
            )

            optimizer = st.selectbox(
                "ä¼˜åŒ–å™¨ (optimizer)",
                options=["SGD", "Adam", "AdamW", "RMSprop"],
                index=0
            )

        # 5. é«˜çº§é€‰é¡¹
        st.subheader("5. é«˜çº§é€‰é¡¹")
        cache = st.checkbox(
            "ä½¿ç”¨ç¼“å­˜ (cache)",
            value=False,
            help="æ˜¯å¦ä½¿ç”¨ç¼“å­˜åŠ é€Ÿè®­ç»ƒ"
        )

        single_cls = st.checkbox(
            "å•ç±»åˆ«æ£€æµ‹ (single_cls)",
            value=False,
            help="æ˜¯å¦ä¸ºå•ç±»åˆ«æ£€æµ‹ä»»åŠ¡"
        )

        amp = st.checkbox(
            "è‡ªåŠ¨æ··åˆç²¾åº¦ (amp)",
            value=True,
            help="æ˜¯å¦ä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ"
        )

        close_mosaic = st.number_input(
            "å…³é—­é©¬èµ›å…‹å¢å¼ºçš„è½®æ•° (close_mosaic)",
            min_value=0,
            max_value=100,
            value=10,
            step=1,
            help="æœ€åNè½®å…³é—­é©¬èµ›å…‹å¢å¼º"
        )

        # 6. è¾“å‡ºé…ç½®
        st.subheader("6. è¾“å‡ºé…ç½®")
        project = st.text_input(
            "é¡¹ç›®ç›®å½• (project)",
            value="runs/train",
            help="è®­ç»ƒç»“æœä¿å­˜çš„æ ¹ç›®å½•"
        )

        name = st.text_input(
            "å®éªŒåç§° (name)",
            value="exp",
            help="è®­ç»ƒå®éªŒçš„åç§°"
        )

        # æäº¤æŒ‰é’®
        submitted = st.form_submit_button("å¼€å§‹è®­ç»ƒ")

        if submitted:
            # éªŒè¯å¿…è¦æ–‡ä»¶æ˜¯å¦ä¸Šä¼ 
            if not model_file:
                st.error("è¯·ä¸Šä¼ æ¨¡å‹é…ç½®æ–‡ä»¶!")
                return

            if not train_images or not val_images:
                st.error("è¯·ä¸Šä¼ è®­ç»ƒå’ŒéªŒè¯å›¾ç‰‡!")
                return

            if not train_labels or not val_labels:
                st.error("è¯·ä¸Šä¼ è®­ç»ƒå’ŒéªŒè¯æ ‡æ³¨!")
                return

            # ä¿å­˜æ¨¡å‹é…ç½®æ–‡ä»¶
            model_config_path = save_uploaded_file(model_file, MODEL_YAML)

            # åˆ›å»ºç›®å½•ç»“æ„
            train_img_dir = os.path.join(TEMP_DIR, "train", "images")
            train_label_dir = os.path.join(TEMP_DIR, "train", "labels")
            val_img_dir = os.path.join(TEMP_DIR, "val", "images")
            val_label_dir = os.path.join(TEMP_DIR, "val", "labels")

            os.makedirs(train_img_dir, exist_ok=True)
            os.makedirs(train_label_dir, exist_ok=True)
            os.makedirs(val_img_dir, exist_ok=True)
            os.makedirs(val_label_dir, exist_ok=True)

            # ä¿å­˜è®­ç»ƒæ•°æ®
            for img in train_images:
                save_uploaded_file(img, os.path.join(train_img_dir, img.name))

            for label in train_labels:
                save_uploaded_file(label, os.path.join(train_label_dir, label.name))

            # ä¿å­˜éªŒè¯æ•°æ®
            for img in val_images:
                save_uploaded_file(img, os.path.join(val_img_dir, img.name))

            for label in val_labels:
                save_uploaded_file(label, os.path.join(val_label_dir, label.name))

            # åˆ›å»ºdata.yaml
            class_list = [name.strip() for name in class_names.split('\n') if name.strip()]
            data_config_path = create_data_yaml(
                os.path.join(TEMP_DIR, "train"),
                os.path.join(TEMP_DIR, "val"),
                class_list
            )

            # æ”¶é›†æ‰€æœ‰å‚æ•°
            params = {
                "model_config": model_config_path,
                "data_config": data_config_path,
                "epochs": epochs,
                "batch": batch,
                "imgsz": imgsz,
                "workers": workers,
                "device": device,
                "optimizer": optimizer,
                "close_mosaic": close_mosaic,
                "cache": cache,
                "single_cls": single_cls,
                "amp": amp,
                "project": project,
                "name": name,
            }

            # æ˜¾ç¤ºé…ç½®ä¿¡æ¯
            st.subheader("è®­ç»ƒé…ç½®æ‘˜è¦")
            st.json(params)

            # è¿è¡Œè®­ç»ƒ
            run_yolo_training(params)

    # æ¸…ç†ä¸´æ—¶ç›®å½•
    if st.button("æ¸…ç†ä¸´æ—¶æ–‡ä»¶"):
        cleanup_temp_dir()
        st.success("ä¸´æ—¶æ–‡ä»¶å·²æ¸…ç†!")



def main():
    global target_class
    # å¯¼èˆªèœå•
    st.sidebar.markdown('<h1 class="sidebar-title">ğŸ§­ å¯¼èˆªèœå•</h1>', unsafe_allow_html=True)
    page = st.sidebar.radio("",
                          ["æ¨¡å‹é€‰æ‹©ï¼ˆå·²æä¾›ï¼‰", "æ¨¡å‹è‡ªå®šä¹‰ï¼ˆè®­ç»ƒï¼‰"],
                          index=0,
                          format_func=lambda x: "ğŸ” å¿«é€Ÿæ£€æµ‹" if x == "æ¨¡å‹é€‰æ‹©ï¼ˆå·²æä¾›ï¼‰" else "ğŸ› ï¸ æ¨¡å‹è®­ç»ƒ"
                          )



    if page == "æ¨¡å‹é€‰æ‹©ï¼ˆå·²æä¾›ï¼‰":
        # å…¨å±€è®¾ç½®åŒºåŸŸï¼ˆæ˜¾ç¤ºåœ¨å¯¼èˆªèœå•ä¸‹æ–¹ï¼‰
        with st.sidebar.expander("âš™ï¸ æ£€æµ‹è®¾ç½®", expanded=True):
            # å•ç±»/å¤šç±»è¯†åˆ«é€‰æ‹©
            detection_mode = st.radio(
                "æ£€æµ‹æ¨¡å¼",
                ["å¤šç±»è¯†åˆ«", "å•ç±»è¯†åˆ«"],
                index=0,
                help="é€‰æ‹©æ˜¯å¦åªæ£€æµ‹ç‰¹å®šç±»åˆ«çš„ç›®æ ‡"
            )

            # ç±»é€‰æ‹©å™¨
            if detection_mode == "å•ç±»è¯†åˆ«":
                # è¿™é‡Œæ›¿æ¢ä¸ºä½ çš„å®é™…ç±»åˆ«åˆ—è¡¨
                class_options = ["cat", "dog", "bird", "teddy bear"]
                target_class = st.selectbox(
                    "é€‰æ‹©è¦è¯†åˆ«çš„ç›®æ ‡ç±»åˆ«",
                    options=class_options,
                    index=0
                )
            else:
                target_class = ["cat", "dog", "bird", "teddy bear"]
        model_usage()
    else:
        model_train()


# è¿è¡Œ Streamlit åº”ç”¨
if __name__ == "__main__":
    main()
